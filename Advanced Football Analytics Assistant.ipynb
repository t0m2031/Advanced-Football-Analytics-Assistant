{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2a7mXWSs24F",
    "outputId": "547ca7ff-736b-46ae-fbf8-06c9694186c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.25.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (5.3.4)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.9.4)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.0)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.4.0)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.7.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0.20240406)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.11.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.20,>=0.19->cohere) (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (2023.6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere) (24.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install openai\n",
    "!pip install tiktoken\n",
    "!pip install  cohere\n",
    "\n",
    "\n",
    "import os, sys, cohere, tiktoken\n",
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjpT1UEKw95I",
    "outputId": "966c0021-41dc-4b67-8216-6fdb3777f66e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eesY4zOTRrvG"
   },
   "source": [
    "# Attempt 1: Simple Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AFHzQHxmwzTz"
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = userdata.get('API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP5jXtCEuzts"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4EiwA23_Wq7",
    "outputId": "4528ffbc-0dc9-4fce-c5e6-5a24146f908a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant created successfully: asst_UyKenpc5wOVaEyekM4Xk29vI\n"
     ]
    }
   ],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Football Assistant\",\n",
    "    instructions=\"Provide the latest information on football matches, player stats, and team news.\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-4\"\n",
    ")\n",
    "print(\"Assistant created successfully:\", assistant.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzAxlljl_WnV",
    "outputId": "740dbf4e-7405-431b-d542-dabb04ebee4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread created successfully: thread_YI1tocS1fPQJc7jAW9a69wR4\n",
      "Please enter your football query: how many champions league does real madrid have?\n",
      "Message added successfully: msg_wd7vjBtYekQe6QtZJgBQXwVa\n"
     ]
    }
   ],
   "source": [
    "# Create a thread for the conversation\n",
    "thread = client.beta.threads.create()\n",
    "print(\"Thread created successfully:\", thread.id)\n",
    "\n",
    "# Get the user's football-related question via input (e.g., in a web form or console input)\n",
    "user_question = input(\"Please enter your football query: \")\n",
    "\n",
    "# Add the user's question to the thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=user_question\n",
    ")\n",
    "print(\"Message added successfully:\", message.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyQmuZu8_Wki",
    "outputId": "6ea8f8f2-86bf-4442-9e7c-9123aa4386c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run initiated successfully: run_2aN7WoT7U4o8OCImeOgZtdkp\n",
      "[TextContentBlock(text=Text(annotations=[], value='Real Madrid has won the UEFA Champions League 13 times. The wins came in the following years: 1956, 1957, 1958, 1959, 1960, 1966, 1998, 2000, 2002, 2014, 2016, 2017, and 2018.'), type='text')]\n",
      "[TextContentBlock(text=Text(annotations=[], value='how many champions league does real madrid have?'), type='text')]\n"
     ]
    }
   ],
   "source": [
    "# Initiate a run to process the user's question\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "print(\"Run initiated successfully:\", run.id)\n",
    "\n",
    "# Monitor the run's completion and fetch messages\n",
    "while True:\n",
    "    lrun = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "    if lrun.status == 'in_progress':\n",
    "        time.sleep(2)  # Wait for 2 seconds before checking again\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Fetch messages from the thread after the run is complete\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "for message in messages.data:\n",
    "    print(message.content)  # Display the assistant's responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEgC4zGNFprj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK6mmXMS3fsy"
   },
   "source": [
    "# Attempt 2: Simple Implementation with Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1-7ZHBOhFpo8"
   },
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Football Assistant\",\n",
    "    instructions= \"Provide the latest information on football matches, player stats, and team news for Real Madrid.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "VHlgK9uvFpmJ"
   },
   "outputs": [],
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"Fixtures\")\n",
    "\n",
    "# files for upload to OpenAI\n",
    "file_paths = [\"/content/drive/MyDrive/Gen_AI_Project_Files/Data.txt\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "sN6CF2EZFpjW"
   },
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fSA9HxyDlrj1",
    "outputId": "98d7681d-49b2-4a85-e6c9-e7513d4b81bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_RMrN77NEovJWDODOS2OtxoGP'])\n"
     ]
    }
   ],
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"/content/drive/MyDrive/Gen_AI_Project_Files/Data.txt\", \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "# Create thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Hello, give me matches where real madrid played barcelona\",\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A07ALwQ9lrgZ",
    "outputId": "84ab9b22-59ab-4272-f28b-72b4fd18629a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found multiple matches where Real Madrid played against Barcelona. Here are a few instances:\n",
      "\n",
      "1. Real Madrid played against FC Barcelona with a result of 3:4[0].\n",
      "2. Another match between Real Madrid and FC Barcelona ended with a score of 1:2[0].\n",
      "3. In a different match, Real Madrid faced FC Barcelona with a score of 3:4[2].\n",
      "4. There was another match between Real Madrid and FC Barcelona with a score of 1:2[2].\n",
      "5. Real Madrid played against FC Barcelona with a score of 2:1[4].\n",
      "\n",
      "These are some of the games between Real Madrid and Barcelona from the provided data.\n"
     ]
    }
   ],
   "source": [
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW3Bltmo2uNC"
   },
   "source": [
    "#Attempt 3: Assistant using a dataset with continuous conversational abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8q90bNUwlrTF",
    "outputId": "d3a4c080-a79a-460f-86a9-7974d673ff96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your football query or type 'exit' to end: real madrid vs barcelona results \n",
      "Assistant's Response: [TextContentBlock(text=Text(annotations=[FileCitationAnnotation(end_index=118, file_citation=FileCitation(file_id='file-dSA7iUy3UrsERvxuaGIyyzYp', quote=None), start_index=106, text='【4:0†source】', type='file_citation'), FileCitationAnnotation(end_index=160, file_citation=FileCitation(file_id='file-dSA7iUy3UrsERvxuaGIyyzYp', quote=None), start_index=148, text='【4:2†source】', type='file_citation')], value='The results of the matches between Real Madrid and Barcelona are as follows:\\n\\n1. Real Madrid 3-4 Barcelona【4:0†source】.\\n2. Real Madrid 1-2 Barcelona【4:2†source】.'), type='text')]\n",
      "Do you have any other queries?\n",
      "Please enter your football query or type 'exit' to end: exit\n",
      "Ending the conversation. Thank you!\n"
     ]
    }
   ],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Football Assistant\",\n",
    "    instructions=\"Provide the latest information on football matches, player stats, and team news for Real Madrid.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")\n",
    "\n",
    "# Create a vector store and upload your data file\n",
    "vector_store = client.beta.vector_stores.create(name=\"Fixtures\")\n",
    "file_path = \"/content/drive/MyDrive/Gen_AI_Project_Files/Data.txt\"\n",
    "\n",
    "# Open file once and reuse the file stream\n",
    "with open(file_path, \"rb\") as file_stream:\n",
    "    # Upload and index files in the vector store\n",
    "    file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id, files=[file_stream]\n",
    "    )\n",
    "\n",
    "# Update the assistant to use the newly created vector store\n",
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")\n",
    "\n",
    "# Create a message file for attachment to messages\n",
    "with open(file_path, \"rb\") as file_stream:\n",
    "    message_file = client.files.create(\n",
    "        file=file_stream, purpose=\"assistants\"\n",
    "    )\n",
    "\n",
    "# Continuous conversation loop with data-enabled responses\n",
    "while True:\n",
    "    user_query = input(\"Please enter your football query or type 'exit' to end: \")\n",
    "    if user_query.lower() in ['exit', 'done']:\n",
    "        print(\"Ending the conversation. Thank you!\")\n",
    "        break\n",
    "    else:\n",
    "        # Create thread and attach the file to the message\n",
    "        thread = client.beta.threads.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_query,\n",
    "                    \"attachments\": [\n",
    "                        {\"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}]}\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Initiate a run to process the user's question with data access\n",
    "        run = client.beta.threads.runs.create_and_poll(\n",
    "            thread_id=thread.id, assistant_id=assistant.id\n",
    "        )\n",
    "\n",
    "        # Retrieve the run's results and print the assistant's response\n",
    "        messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "        for message in messages:\n",
    "            if message.role == 'assistant':  # Filter out only assistant's messages\n",
    "                print(\"Assistant's Response:\", message.content)\n",
    "                break\n",
    "        print(\"Do you have any other queries?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GblEDACWThG"
   },
   "source": [
    "# Attempt 4: Assistant using a dataset with continuous conversational abilities with complex prompts such as RAG and COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZWuQqahlrQS",
    "outputId": "d5792393-4790-4dfb-f095-e65dc33cf822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your football query or type 'exit' to end: real madrid vs barcelona latest match results \n",
      "Assistant's CoT and RAG Response: [TextContentBlock(text=Text(annotations=[FileCitationAnnotation(end_index=212, file_citation=FileCitation(file_id='file-xkEfZisY1ZN3m69HVRAXFr0K', quote=None), start_index=200, text='【4:0†source】', type='file_citation')], value='The latest match result between Real Madrid and FC Barcelona was a victory for Real Madrid with a score of 3-1. This victory indicates a strong performance by Real Madrid against their rival Barcelona【4:0†source】.\\n\\nComparing historical data between Real Madrid and Barcelona from the provided match results shows that both teams have had competitive matches over time. Real Madrid has had victories against Barcelona in various matches, showcasing their strength in these encounters. It would be interesting to analyze more matches between these two teams to identify patterns and trends in their performances and outcomes over time.'), type='text')]\n",
      "Do you have any other queries?\n",
      "Please enter your football query or type 'exit' to end: no thanks \n",
      "Assistant's CoT and RAG Response: [TextContentBlock(text=Text(annotations=[], value='The provided data contains match results for different teams in various competitions. By analyzing these results, we can derive insights into team performance and outcomes. To delve deeper into the analysis and provide meaningful comparisons and trends, we can look into specific team performances, goal differentials, win-loss ratios, and more. Would you like to focus on a particular team or aspect of the data for a detailed analysis?'), type='text')]\n",
      "Do you have any other queries?\n",
      "Please enter your football query or type 'exit' to end: exit\n",
      "Ending the conversation. Thank you!\n"
     ]
    }
   ],
   "source": [
    "# Create an assistant with capabilities for file search\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Football Assistant\",\n",
    "    instructions=\"Analyze and provide detailed insights on football matches, player stats, and team news for Real Madrid, including historical data comparison and trend analysis.\",\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")\n",
    "\n",
    "# Create a vector store to enable efficient data search\n",
    "vector_store = client.beta.vector_stores.create(name=\"Fixtures\")\n",
    "file_path = \"/content/drive/MyDrive/Gen_AI_Project_Files/Data.txt\"\n",
    "\n",
    "# Open the file once and use the file stream to upload and index the file in the vector store\n",
    "with open(file_path, \"rb\") as file_stream:\n",
    "    file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id, files=[file_stream]\n",
    "    )\n",
    "\n",
    "# Update the assistant to use the newly created vector store\n",
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")\n",
    "\n",
    "# Create a message file for attachment to messages using the same data file\n",
    "with open(file_path, \"rb\") as file_stream:\n",
    "    message_file = client.files.create(\n",
    "        file=file_stream, purpose=\"assistants\"\n",
    "    )\n",
    "\n",
    "def process_query_with_cot_rag(user_query):\n",
    "    \"\"\"Process user queries using Chain of Thought and Retrieval-Augmented Generation.\"\"\"\n",
    "    # Create a thread with an enhanced prompt for CoT and attach the file\n",
    "    thread = client.beta.threads.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"First, let's retrieve relevant data for: {user_query}. Analyze the data to extract meaningful insights and comparisons.\",\n",
    "                \"attachments\": [{\"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}]}],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Initiate a run to process the user's question with data access\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id, assistant_id=assistant.id\n",
    "    )\n",
    "\n",
    "    # Retrieve and format the assistant's responses\n",
    "    messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "    for message in messages:\n",
    "        if message.role == 'assistant':\n",
    "            print(\"Assistant's CoT and RAG Response:\", message.content)\n",
    "            break\n",
    "\n",
    "# Continuous conversation loop with data-enabled responses\n",
    "while True:\n",
    "    user_query = input(\"Please enter your football query or type 'exit' to end: \")\n",
    "    if user_query.lower() in ['exit', 'done', 'no']:\n",
    "        print(\"Ending the conversation. Thank you!\")\n",
    "        break\n",
    "    else:\n",
    "        process_query_with_cot_rag(user_query)\n",
    "        print(\"Do you have any other queries?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu5gtyVx31G2"
   },
   "source": [
    "Sample COT and RAG examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZSK2i-YUly-"
   },
   "outputs": [],
   "source": [
    "# Assume all imports and basic setup as previously defined\n",
    "\n",
    "def engage_fan_conversation(query):\n",
    "    if 'prediction' in query.lower():\n",
    "        return \"What's your prediction for today's game? Here's some stats to help you decide!\"\n",
    "    elif 'trivia' in query.lower():\n",
    "        return \"Time for some trivia! Who scored the winning goal in last year's fixture?\"\n",
    "    elif 'learn' in query.lower():\n",
    "        return \"Let's learn about football tactics. Today's topic: Total Football. Ready to start?\"\n",
    "\n",
    "# Enhance the conversation loop\n",
    "while True:\n",
    "    user_query = input(\"Please enter your football query or type 'exit' to end: \")\n",
    "    if user_query.lower() in ['exit', 'done']:\n",
    "        print(\"Ending the conversation. Thank you!\")\n",
    "        break\n",
    "    else:\n",
    "        engagement_response = engage_fan_conversation(user_query)\n",
    "        print(\"Assistant's Response:\", engagement_response)\n",
    "        print(\"Do you have any other queries?\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
